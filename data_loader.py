"""
–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è CT-RATE –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ DICOM —Ñ–∞–π–ª–æ–≤
"""
import os
import torch
import numpy as np
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from datasets import load_dataset
import nibabel as nib
import pydicom
from typing import List, Dict, Tuple, Optional, Union
import cv2
from sklearn.preprocessing import StandardScaler
import albumentations as A
from albumentations.pytorch import ToTensorV2
import warnings
warnings.filterwarnings("ignore")

from config import PATHOLOGY_LABELS, data_config, training_config

class CTVolumeDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ CT –æ–±—ä–µ–º–æ–≤ –∏–∑ CT-RATE"""
    
    def __init__(self, 
                 split: str = "train",
                 transform: Optional[A.Compose] = None,
                 normalize: bool = True):
        """
        Args:
            split: –†–∞–∑–¥–µ–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ ("train" –∏–ª–∏ "validation")
            transform: –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        """
        self.split = split
        self.transform = transform
        self.normalize = normalize
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º CT-RATE –¥–∞—Ç–∞—Å–µ—Ç, —Ä–∞–∑–¥–µ–ª: {split}")
        
        # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        self.dataset = None
        self.is_synthetic = False
        
        try:
            print("–ó–∞–≥—Ä—É–∂–∞–µ–º CT-RATE –¥–∞—Ç–∞—Å–µ—Ç...")
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            self.dataset = load_dataset(data_config.ct_rate_dataset_name, 'labels', split=split)
            
            print(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ! –†–∞–∑–º–µ—Ä: {len(self.dataset)}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–µ—Ä–≤–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞
            if len(self.dataset) > 0:
                first_sample = self.dataset[0]
                print(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–µ—Ä–≤–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞: {type(first_sample)}")
                if hasattr(first_sample, 'keys'):
                    print(f"üîç –ö–ª—é—á–∏ –ø–µ—Ä–≤–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞: {list(first_sample.keys())}")
                else:
                    print(f"üîç –ü–µ—Ä–≤—ã–π –æ–±—Ä–∞–∑–µ—Ü: {first_sample}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CT-RATE: {str(e)[:200]}...")
            print("üîÑ –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
            self._create_synthetic_dataset(1000)  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            self.is_synthetic = True
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        try:
            self.metadata = self.dataset.to_pandas()
        except:
            self.metadata = None
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.dataset)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    def _create_synthetic_dataset(self, num_samples: int):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print(f"üîß –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å {num_samples} –æ–±—Ä–∞–∑—Ü–∞–º–∏...")
        
        # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
        synthetic_data = []
        for i in range(num_samples):
            # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π CT –æ–±—ä–µ–º
            volume_shape = (64, 64, 64)  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä
            ct_volume = np.random.randn(*volume_shape).astype(np.float32)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –º–µ—Ç–∫–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π
            labels = np.random.randint(0, 2, len(PATHOLOGY_LABELS)).astype(np.float32)
            
            synthetic_data.append({
                'image': ct_volume,
                'labels': labels,
                'patient_id': f"synthetic_{i}",
                'scan_id': f"scan_{i}",
                'reconstruction_id': f"recon_{i}"
            })
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç-–æ–±–µ—Ä—Ç–∫—É
        class SyntheticDataset:
            def __init__(self, data):
                self.data = data
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                return self.data[idx]
        
        self.dataset = SyntheticDataset(synthetic_data)
        print("‚úÖ –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω!")
        
    def __len__(self):
        return len(self.dataset)
    
    def __getitem__(self, idx):
        sample = self.dataset[idx]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ sample —Å–ª–æ–≤–∞—Ä–µ–º
        if not isinstance(sample, dict):
            print(f"‚ö†Ô∏è –û–±—Ä–∞–∑–µ—Ü {idx} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä–µ–º: {type(sample)}")
            # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            volume = np.random.randn(64, 64, 64).astype(np.float32)
            labels = np.zeros(len(PATHOLOGY_LABELS), dtype=np.float32)
            return {
                'volume': torch.FloatTensor(volume),
                'labels': torch.FloatTensor(labels),
                'file_path': f"synthetic_{idx}"
            }
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –æ–±—Ä–∞–∑—Ü–æ–≤)
        if idx < 3:
            available_keys = list(sample.keys()) if hasattr(sample, 'keys') else []
            print(f"üîç –û–±—Ä–∞–∑–µ—Ü {idx}: –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {available_keys}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –º–µ—Ç–æ–∫
            pathology_examples = {}
            for pathology in PATHOLOGY_LABELS[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø–∞—Ç–æ–ª–æ–≥–∏–π
                if pathology in sample:
                    pathology_examples[pathology] = sample[pathology]
            if pathology_examples:
                print(f"üîç –ü—Ä–∏–º–µ—Ä—ã –º–µ—Ç–æ–∫: {pathology_examples}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–ª—é—á –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å 'image', 'ct', 'volume', etc.)
        image_key = None
        possible_keys = ['image', 'ct', 'volume', 'data', 'scan', 'VolumeName']
        
        for key in possible_keys:
            if key in sample:
                image_key = key
                break
        
        if image_key is None:
            # –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤—ã–≤–æ–¥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            available_keys = list(sample.keys()) if hasattr(sample, 'keys') else []
            print(f"‚ö†Ô∏è –ö–ª—é—á –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω. –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–ª—é—á–∏: {available_keys}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–ª—é—á –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            if available_keys:
                image_key = available_keys[0]
            else:
                volume = np.random.randn(64, 64, 64).astype(np.float32)
                labels = np.zeros(len(PATHOLOGY_LABELS), dtype=np.float32)
                return {
                    'volume': torch.FloatTensor(volume),
                    'labels': torch.FloatTensor(labels),
                    'file_path': f"synthetic_{idx}"
                }
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º CT –æ–±—ä–µ–º
            ct_volume = sample[image_key]
            if isinstance(ct_volume, np.ndarray):
                volume = ct_volume
            else:
                # –ï—Å–ª–∏ —ç—Ç–æ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
                try:
                    volume = nib.load(ct_volume).get_fdata()
                except:
                    # Fallback –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                    volume = np.random.randn(64, 64, 64).astype(np.float32)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
            if self.normalize:
                volume = self._normalize_intensity(volume)
            
            # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ
            volume = self._resize_volume(volume, data_config.dicom_resize_to)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π
            labels = self._get_labels(sample)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.transform:
                volume = self._apply_augmentation(volume)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
            volume = torch.FloatTensor(volume).unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞–Ω–∞–ª
            
            return {
                'volume': volume,
                'labels': torch.FloatTensor(labels),
                'patient_id': sample.get('patient_id', idx),
                'scan_id': sample.get('scan_id', ''),
                'reconstruction_id': sample.get('reconstruction_id', '')
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–±—Ä–∞–∑—Ü–∞ {idx}: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            volume = np.random.randn(64, 64, 64).astype(np.float32)
            labels = np.zeros(len(PATHOLOGY_LABELS), dtype=np.float32)
            return {
                'volume': torch.FloatTensor(volume),
                'labels': torch.FloatTensor(labels),
                'file_path': f"error_{idx}"
            }
    
    def _normalize_intensity(self, volume: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏ CT –æ–±—ä–µ–º–∞"""
        # –û–±—Ä–µ–∑–∫–∞ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        volume = np.clip(volume, -1000, 1000)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ –¥–∏–∞–ø–∞–∑–æ–Ω—É [0, 1]
        volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-8)
        
        return volume
    
    def _resize_volume(self, volume: np.ndarray, target_size: Tuple[int, int, int]) -> np.ndarray:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—ä–µ–º–∞"""
        if volume.shape == target_size:
            return volume
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞
        resized = np.zeros(target_size)
        
        # –ü—Ä–æ—Å—Ç–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É —Å—Ä–µ–∑—É
        for i in range(target_size[2]):
            z_idx = int(i * volume.shape[2] / target_size[2])
            slice_2d = volume[:, :, z_idx]
            resized_slice = cv2.resize(slice_2d, (target_size[0], target_size[1]))
            resized[:, :, i] = resized_slice
        
        return resized
    
    def _get_labels(self, sample: Dict) -> List[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –ø–∞—Ç–æ–ª–æ–≥–∏–π"""
        labels = [0.0] * len(PATHOLOGY_LABELS)
        
        # –î–ª—è CT-RATE –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–µ—Ç–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–ª—é—á–∞—Ö
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–π –∫–ª—é—á –ø–∞—Ç–æ–ª–æ–≥–∏–∏ –≤ sample
        for i, pathology in enumerate(PATHOLOGY_LABELS):
            if pathology in sample:
                # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç–∫–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å 0/1, True/False, –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞)
                label_value = sample[pathology]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float
                if isinstance(label_value, (int, float)):
                    labels[i] = float(label_value)
                elif isinstance(label_value, bool):
                    labels[i] = 1.0 if label_value else 0.0
                elif isinstance(label_value, str):
                    # –î–ª—è —Å—Ç—Ä–æ–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    if label_value.lower() in ['true', '1', 'yes', 'positive']:
                        labels[i] = 1.0
                    elif label_value.lower() in ['false', '0', 'no', 'negative']:
                        labels[i] = 0.0
                    else:
                        labels[i] = 0.0  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # Fallback: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–ª—é—á 'labels'
        if 'labels' in sample:
            sample_labels = sample['labels']
            if isinstance(sample_labels, list):
                for label in sample_labels:
                    if label in PATHOLOGY_LABELS:
                        idx = PATHOLOGY_LABELS.index(label)
                        labels[idx] = 1.0
            elif isinstance(sample_labels, str):
                if sample_labels in PATHOLOGY_LABELS:
                    idx = PATHOLOGY_LABELS.index(sample_labels)
                    labels[idx] = 1.0
        
        return labels
    
    def _apply_augmentation(self, volume: np.ndarray) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π –∫ –æ–±—ä–µ–º—É"""
        # –ü—Ä–æ—Å—Ç—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –¥–ª—è 3D –¥–∞–Ω–Ω—ã—Ö
        if np.random.random() < 0.5:
            # –ü–æ–≤–æ—Ä–æ—Ç
            angle = np.random.uniform(-10, 10)
            for i in range(volume.shape[2]):
                slice_2d = volume[:, :, i]
                M = cv2.getRotationMatrix2D((slice_2d.shape[1]/2, slice_2d.shape[0]/2), angle, 1)
                volume[:, :, i] = cv2.warpAffine(slice_2d, M, (slice_2d.shape[1], slice_2d.shape[0]))
        
        if np.random.random() < 0.3:
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞
            noise = np.random.normal(0, 0.01, volume.shape)
            volume = volume + noise
            volume = np.clip(volume, 0, 1)
        
        return volume

class DICOMDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ DICOM —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self, 
                 dicom_paths: List[str],
                 labels: Optional[List[List[float]]] = None,
                 transform: Optional[A.Compose] = None,
                 normalize: bool = True):
        """
        Args:
            dicom_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ DICOM —Ñ–∞–π–ª–∞–º
            labels: –ú–µ—Ç–∫–∏ –ø–∞—Ç–æ–ª–æ–≥–∏–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            transform: –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            normalize: –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        """
        self.dicom_paths = dicom_paths
        self.labels = labels or [[0.0] * len(PATHOLOGY_LABELS)] * len(dicom_paths)
        self.transform = transform
        self.normalize = normalize
        
    def __len__(self):
        return len(self.dicom_paths)
    
    def __getitem__(self, idx):
        dicom_path = self.dicom_paths[idx]
        labels = self.labels[idx]
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º DICOM —Ñ–∞–π–ª
        volume = self._load_dicom_volume(dicom_path)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏
        if self.normalize:
            volume = self._normalize_intensity(volume)
        
        # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        volume = self._resize_volume(volume, data_config.dicom_resize_to)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        if self.transform:
            volume = self._apply_augmentation(volume)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
        volume = torch.FloatTensor(volume).unsqueeze(0)
        
        return {
            'volume': volume,
            'labels': torch.FloatTensor(labels),
            'file_path': dicom_path
        }
    
    def _load_dicom_volume(self, dicom_path: str) -> np.ndarray:
        """–ó–∞–≥—Ä—É–∑–∫–∞ DICOM –æ–±—ä–µ–º–∞"""
        if os.path.isfile(dicom_path):
            # –û–¥–∏–Ω–æ—á–Ω—ã–π —Ñ–∞–π–ª
            ds = pydicom.dcmread(dicom_path)
            return ds.pixel_array
        else:
            # –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏
            dicom_files = []
            for file in os.listdir(dicom_path):
                if file.lower().endswith(('.dcm', '.dicom')):
                    dicom_files.append(os.path.join(dicom_path, file))
            
            dicom_files.sort()
            
            volumes = []
            for file_path in dicom_files:
                ds = pydicom.dcmread(file_path)
                volumes.append(ds.pixel_array)
            
            return np.stack(volumes, axis=2)
    
    def _normalize_intensity(self, volume: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç–∏"""
        volume = np.clip(volume, -1000, 1000)
        volume = (volume - volume.min()) / (volume.max() - volume.min() + 1e-8)
        return volume
    
    def _resize_volume(self, volume: np.ndarray, target_size: Tuple[int, int, int]) -> np.ndarray:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—ä–µ–º–∞"""
        if volume.shape == target_size:
            return volume
        
        resized = np.zeros(target_size)
        for i in range(target_size[2]):
            z_idx = int(i * volume.shape[2] / target_size[2])
            slice_2d = volume[:, :, z_idx]
            resized_slice = cv2.resize(slice_2d, (target_size[0], target_size[1]))
            resized[:, :, i] = resized_slice
        
        return resized
    
    def _apply_augmentation(self, volume: np.ndarray) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π"""
        if np.random.random() < 0.5:
            angle = np.random.uniform(-10, 10)
            for i in range(volume.shape[2]):
                slice_2d = volume[:, :, i]
                M = cv2.getRotationMatrix2D((slice_2d.shape[1]/2, slice_2d.shape[0]/2), angle, 1)
                volume[:, :, i] = cv2.warpAffine(slice_2d, M, (slice_2d.shape[1], slice_2d.shape[0]))
        
        if np.random.random() < 0.3:
            noise = np.random.normal(0, 0.01, volume.shape)
            volume = volume + noise
            volume = np.clip(volume, 0, 1)
        
        return volume

def get_data_loaders(batch_size: int = 8, 
                    num_workers: int = 0,  # –û—Ç–∫–ª—é—á–∞–µ–º multiprocessing –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    use_augmentation: bool = True) -> Tuple[DataLoader, DataLoader]:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"""
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
    transform = None
    if use_augmentation:
        transform = A.Compose([
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        ])
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
    train_dataset = CTVolumeDataset(split="train", transform=transform)
    val_dataset = CTVolumeDataset(split="validation", transform=None)
    
    # –°–æ–∑–¥–∞–µ–º DataLoader'—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=False,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        drop_last=False
    )
    
    return train_loader, val_loader

def create_dicom_loader(dicom_paths: List[str], 
                       labels: Optional[List[List[float]]] = None,
                       batch_size: int = 8,
                       num_workers: int = 4) -> DataLoader:
    """–°–æ–∑–¥–∞–Ω–∏–µ DataLoader –¥–ª—è DICOM —Ñ–∞–π–ª–æ–≤"""
    
    dataset = DICOMDataset(dicom_paths, labels)
    
    loader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=False
    )
    
    return loader

