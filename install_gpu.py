#!/usr/bin/env python3
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPU
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –≤–µ—Ä—Å–∏—é
"""
import subprocess
import sys
import platform
import os

def run_command(command, description):
    """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã —Å –≤—ã–≤–æ–¥–æ–º"""
    print(f"\nüîÑ {description}...")
    print(f"–ö–æ–º–∞–Ω–¥–∞: {command}")
    
    try:
        result = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)
        print(f"‚úÖ {description} –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        if result.stdout:
            print(f"–í—ã–≤–æ–¥: {result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ {description}")
        print(f"–ö–æ–¥ –æ—à–∏–±–∫–∏: {e.returncode}")
        if e.stdout:
            print(f"–í—ã–≤–æ–¥: {e.stdout}")
        if e.stderr:
            print(f"–û—à–∏–±–∫–∞: {e.stderr}")
        return False

def detect_gpu_type():
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ GPU"""
    print("üîç –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø GPU...")
    
    try:
        import wmi
        c = wmi.WMI()
        
        gpus = []
        for gpu in c.Win32_VideoController():
            if gpu.Name:
                gpus.append(gpu.Name.lower())
                print(f"–ù–∞–π–¥–µ–Ω GPU: {gpu.Name}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø GPU
        if any('nvidia' in gpu or 'geforce' in gpu or 'rtx' in gpu or 'gtx' in gpu for gpu in gpus):
            return "nvidia"
        elif any('amd' in gpu or 'radeon' in gpu or 'rx' in gpu for gpu in gpus):
            return "amd"
        else:
            return "unknown"
            
    except ImportError:
        print("‚ö†Ô∏è  –ú–æ–¥—É–ª—å wmi –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É GPU")
        return "unknown"
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ GPU: {e}")
        return "unknown"

def install_pytorch_cuda():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA"""
    print("\nüöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NVIDIA CUDA...")
    
    commands = [
        "python -m pip install --upgrade pip",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121",
        "pip install -r requirements.txt"
    ]
    
    success = True
    for command in commands:
        if not run_command(command, f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {command.split()[2] if len(command.split()) > 2 else command}"):
            success = False
            break
    
    return success

def install_pytorch_directml():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π DirectML (AMD –Ω–∞ Windows)"""
    print("\nüöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AMD DirectML...")
    
    commands = [
        "python -m pip install --upgrade pip",
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu",
        "pip install torch-directml",
        "pip install -r requirements.txt"
    ]
    
    success = True
    for command in commands:
        if not run_command(command, f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {command.split()[2] if len(command.split()) > 2 else command}"):
            success = False
            break
    
    return success

def install_pytorch_rocm():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ROCm (AMD –Ω–∞ Linux)"""
    print("\nüöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AMD ROCm...")
    
    commands = [
        "python -m pip install --upgrade pip",
        "pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm5.6",
        "pip install -r requirements.txt"
    ]
    
    success = True
    for command in commands:
        if not run_command(command, f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {command.split()[2] if len(command.split()) > 2 else command}"):
            success = False
            break
    
    return success

def install_pytorch_cpu():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch —Ç–æ–ª—å–∫–æ –¥–ª—è CPU"""
    print("\nüöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch –¥–ª—è CPU...")
    
    commands = [
        "python -m pip install --upgrade pip",
        "pip install torch torchvision torchaudio",
        "pip install -r requirements.txt"
    ]
    
    success = True
    for command in commands:
        if not run_command(command, f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ: {command.split()[2] if len(command.split()) > 2 else command}"):
            success = False
            break
    
    return success

def test_installation():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏"""
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É...")
    
    test_code = """
import torch
import sys

print(f"Python –≤–µ—Ä—Å–∏—è: {sys.version}")
print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch.cuda.device_count()}")
    for i in range(torch.cuda.device_count()):
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {i}: {torch.cuda.get_device_name(i)}")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º DirectML (Windows)
try:
    import torch_directml
    print(f"DirectML –¥–æ—Å—Ç—É–ø–µ–Ω: {torch_directml.device_count() > 0}")
    if torch_directml.device_count() > 0:
        print(f"DirectML —É—Å—Ç—Ä–æ–π—Å—Ç–≤: {torch_directml.device_count()}")
except ImportError:
    print("DirectML: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º ROCm (Linux)
if hasattr(torch.version, 'hip'):
    print(f"ROCm –≤–µ—Ä—Å–∏—è: {torch.version.hip}")
else:
    print("ROCm –≤–µ—Ä—Å–∏—è: –ù–µ –¥–æ—Å—Ç—É–ø–Ω–∞")

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–∞
x = torch.randn(2, 3)
print(f"–¢–µ–Ω–∑–æ—Ä —Å–æ–∑–¥–∞–Ω: {x.shape}")

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏—é
y = torch.randn(2, 3)
z = x + y
print(f"–û–ø–µ—Ä–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: {z.shape}")

print("‚úÖ PyTorch —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
"""
    
    try:
        exec(test_code)
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        return False

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("=" * 60)
    print("–£–ù–ò–í–ï–†–°–ê–õ–¨–ù–ê–Ø –£–°–¢–ê–ù–û–í–ö–ê PYTORCH –° –ü–û–î–î–ï–†–ñ–ö–û–ô GPU")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python –≤–µ—Ä—Å–∏—é
    if sys.version_info < (3, 8):
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è Python 3.8 –∏–ª–∏ –≤—ã—à–µ")
        return False
    
    print(f"‚úÖ Python –≤–µ—Ä—Å–∏—è: {sys.version}")
    print(f"‚úÖ –û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞: {platform.system()} {platform.version()}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –∏ GPU
    system = platform.system()
    gpu_type = "unknown"
    
    if system == "Windows":
        gpu_type = detect_gpu_type()
    elif system == "Linux":
        # –ù–∞ Linux –ø—Ä–æ–≤–µ—Ä—è–µ–º ROCm
        if os.path.exists("/opt/rocm") or os.environ.get("ROCM_PATH"):
            gpu_type = "amd"
        else:
            gpu_type = "unknown"
    
    print(f"‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω GPU: {gpu_type.upper()}")
    
    # –í—ã–±–∏—Ä–∞–µ–º –º–µ—Ç–æ–¥ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
    if system == "Windows":
        if gpu_type == "nvidia":
            print("\nüéØ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π NVIDIA CUDA")
            success = install_pytorch_cuda()
        elif gpu_type == "amd":
            print("\nüéØ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AMD DirectML")
            success = install_pytorch_directml()
        else:
            print("\nüéØ GPU –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch –¥–ª—è CPU")
            success = install_pytorch_cpu()
    elif system == "Linux":
        if gpu_type == "amd":
            print("\nüéØ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π AMD ROCm")
            success = install_pytorch_rocm()
        else:
            print("\nüéØ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch –¥–ª—è CPU")
            success = install_pytorch_cpu()
    else:
        print("\nüéØ –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –û–°, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º PyTorch –¥–ª—è CPU")
        success = install_pytorch_cpu()
    
    if not success:
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ PyTorch")
        return False
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É
    if not test_installation():
        print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∏")
        return False
    
    print("\n" + "=" * 60)
    print("‚úÖ –£–°–¢–ê–ù–û–í–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–ï–®–ù–û!")
    print("=" * 60)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
    print("\nüöÄ –¢–µ–ø–µ—Ä—å –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–µ–∫—Ç:")
    print("python main.py train --epochs 10")
    print("\nüåê –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å:")
    print("python web_interface.py")
    print("\nüß™ –ò–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å GPU:")
    print("python test_amd_gpu.py")
    
    print("\nüìã –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:")
    print("‚Ä¢ Windows + NVIDIA GPU ‚Üí CUDA")
    print("‚Ä¢ Windows + AMD GPU ‚Üí DirectML")
    print("‚Ä¢ Linux + AMD GPU ‚Üí ROCm")
    print("‚Ä¢ –õ—é–±–∞—è –û–° –±–µ–∑ GPU ‚Üí CPU")
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
